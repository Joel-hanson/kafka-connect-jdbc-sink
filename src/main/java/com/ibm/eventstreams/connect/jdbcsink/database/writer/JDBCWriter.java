/*
 *
 * Copyright 2020 IBM Corporation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */

package com.ibm.eventstreams.connect.jdbcsink.database.writer;

import com.ibm.eventstreams.connect.jdbcsink.JDBCSinkTask;
import com.ibm.eventstreams.connect.jdbcsink.database.datasource.IDataSource;
import org.apache.kafka.connect.data.Field;
import org.apache.kafka.connect.data.Schema;
import org.apache.kafka.connect.data.Struct;
import org.apache.kafka.connect.sink.SinkRecord;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.sql.Connection;
import java.sql.DatabaseMetaData;
import java.sql.SQLException;
import java.sql.PreparedStatement;
import java.sql.BatchUpdateException;
import java.sql.ResultSet;
import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.stream.Collectors;

public class JDBCWriter implements IDatabaseWriter {

    private static final Logger LOGGER = LoggerFactory.getLogger(JDBCSinkTask.class);

    private final IDataSource dataSource;

    public JDBCWriter(final IDataSource dataSource) {
        this.dataSource = dataSource;
    }

    private boolean doesTableExist(final Connection connection, final String tableName) throws SQLException {
        final String[] tableParts = tableName.split("\\.");
        final DatabaseMetaData dbm = connection.getMetaData();
        final ResultSet table = dbm.getTables(null, tableParts[0], tableParts[1], null);
        return table.next();
    }

    // TODO: verify will work for all database flavors
    private String getDatabaseFieldType(final Schema.Type type) {
        switch (type) {
            case INT8:
            case INT16:
            case INT32:
                return "INTEGER";
            case INT64:
                return "BIGINT";
            case FLOAT32:
            case FLOAT64:
                return "FLOAT";
            case BOOLEAN:
                return "BOOLEAN";
            default:
                return "VARCHAR(255)";
        }
    }

    public void createTable(final Connection connection, final String tableName, final Schema schema)
            throws SQLException {

        // TODO: verify will work for all database flavors
        final String createStatement = "CREATE TABLE ? (?)";

        final List<String> fieldDatabaseDefinitions = new ArrayList<>();
        // TODO: verify will work for all database flavors
        fieldDatabaseDefinitions.add("id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY");
        for (final Field field : schema.fields()) {
            final String fieldName = field.name();
            final Schema.Type fieldType = field.schema().type();
            // TODO: verify will work for all database flavors
            final String nullable = field.schema().isOptional() ? "" : " NOT NULL DEFAULT";
            fieldDatabaseDefinitions
                    .add(String.format("%s %s%s", fieldName, getDatabaseFieldType(fieldType), nullable));
        }

        final PreparedStatement pstmt = connection.prepareStatement(createStatement);
        pstmt.setString(1, tableName);
        pstmt.setString(2, String.join(", ", fieldDatabaseDefinitions));
        LOGGER.info("TABLE " + tableName + " has been created");
        pstmt.execute();

        pstmt.close();
    }

    // TODO: encode maps and lists as strings
    private String encodeValueForQuery(final Object value) {
        return value instanceof String ? String.format("'%s'", value.toString()) : value.toString();
    }

    @Override
    public void insert(final String tableName, final Collection<SinkRecord> records) throws SQLException {
        Connection connection = null;
        // TODO: need an SQL statement builder with potential variations depending on
        // the platform
        final String insertStatement = "INSERT INTO ?(?) VALUES (?)";
        try {
            connection = this.dataSource.getConnection();
            final PreparedStatement pstmt = connection.prepareStatement(insertStatement);

            if (!doesTableExist(connection, tableName)) {
                LOGGER.info("Table not found. Creating table: " + tableName);
                createTable(connection, tableName, records.iterator().next().valueSchema());
            }

            final List<String> fieldNames = records.iterator().next().valueSchema().fields().stream().map(Field::name)
                    .collect(Collectors.toList());

            for (final SinkRecord record : records) {
                final Struct recordValue = (Struct) record.value();

                LOGGER.debug(" --- Record Schema --- ");
                LOGGER.debug(record.valueSchema().toString());
                LOGGER.debug(" --- Record Value --- ");
                LOGGER.debug(record.value().toString());
                LOGGER.debug(" --- Record Headers --- ");
                LOGGER.debug(record.headers().toString());

                final List<String> fieldValues = fieldNames.stream()
                        .map(fieldName -> encodeValueForQuery(recordValue.get(fieldName))).collect(Collectors.toList());

                LOGGER.debug("TableFields: " + fieldNames.size());
                LOGGER.debug("TableFields value: " + fieldNames.toString());
                LOGGER.debug("DataFields: " + fieldValues.size());
                LOGGER.debug("DataFields value: " + fieldValues.toString());

                final String listTableFields = String.join(", ", fieldNames);
                final String listDataFields = String.join(", ", fieldValues);

                pstmt.setString(1, tableName);
                pstmt.setString(2, listTableFields);
                pstmt.setString(3, listDataFields);

                pstmt.addBatch();
                LOGGER.debug("Final prepared statement: '{}' //", insertStatement);
            }

            pstmt.executeBatch();
            pstmt.close();

        } catch (final BatchUpdateException batchUpdateException) {
            // TODO: write failed records from batch to kafka topic?
            LOGGER.error("SOME OPERATIONS IN BATCH FAILED");
            LOGGER.error(batchUpdateException.toString());
        } catch (final SQLException sQLException) {
            LOGGER.error(sQLException.toString());
            throw sQLException;
        } finally {
            if (connection != null) {
                connection.close();
            }
        }
    }
}
